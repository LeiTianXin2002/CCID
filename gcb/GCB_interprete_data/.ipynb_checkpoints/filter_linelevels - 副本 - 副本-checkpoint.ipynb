{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad6ca09",
   "metadata": {},
   "source": [
    "此 .ipynb为按照行级挑选最有代表性的代码注释对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e3fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import LayerConductance, LayerIntegratedGradients\n",
    "from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer\n",
    "\n",
    "# 解决服务器挂掉的问题\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff062613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device( \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf31e07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 1024\n",
    "MAX_EPOCHS = 10\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_CLASSES = 2\n",
    "WEIGTH_DECAY = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8e3dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_test_data():\n",
    "    test_param = pd.read_json(\"D:/BERT_learing/code_comment_inconsistency_detection/data/param/test.json\")\n",
    "    test_return = pd.read_json(\"D:/BERT_learing/code_comment_inconsistency_detection/data/return/test.json\")\n",
    "    test_summary = pd.read_json(\"D:/BERT_learing/code_comment_inconsistency_detection/data/summary/test.json\")\n",
    "    test_df = pd.concat([test_summary,test_param, test_return], axis=0)\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "    return test_df\n",
    "test_df = retrieve_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e4d12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lines_count(df):\n",
    "    line_counts = []\n",
    "    for i in range(len(df)):\n",
    "        string = df.loc[i]['new_code_raw']\n",
    "        line_count = len(string.split('\\n'))\n",
    "        line_counts.append(line_count)\n",
    "    df['line_counts'] = line_counts\n",
    "    return df\n",
    "test_df = get_lines_count(test_df)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5551f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_last(df):\n",
    "    for i in range(len(df)):\n",
    "        string = df.loc[i]['old_comment_raw']\n",
    "        df['old_comment_raw'][i] = string.rstrip('.')\n",
    "    return df\n",
    "test_df = remove_last(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42afc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = test_df\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf598e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('save_GCBmodel.pt',map_location=torch.device('cuda:0'))\n",
    "model = torch.load('D:/BERT_learing/CCDP/for_captum/save_model/save_GCBmodel.pt',map_location=torch.device('cpu'))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76cca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())  #输出为True，则安装无误\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0b8ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba07f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"D:/BERT_learing/code_comment_inconsistency_detection/graphcodebert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea322f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.roberta.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6de6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.roberta.encoder.layer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79e32eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ca9012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f64e8e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c241dc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict和squad_pos_forward_func可以合成一个\n",
    "def predict(inputs, position_ids=None, attention_mask=None):\n",
    "    output = model(inputs,\n",
    "                   position_ids=position_ids,\n",
    "                  attention_mask=attention_mask )\n",
    "    \n",
    "    prediction = output.logits\n",
    "    prediction_1 = nn.functional.softmax(prediction, dim=1)\n",
    "    prediction = prediction_1.max(1).values\n",
    "    out = torch.argmax(prediction_1, dim=-1)\n",
    "    # prediction：每个输入样本的最大预测概率。\n",
    "    # out：预测的类别标签。\n",
    "    # prediction_1：所有类别的预测概率。    \n",
    "    return prediction,out,prediction_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4de032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squad_pos_forward_func(inputs,position_ids=None, attention_mask=None, position=0):\n",
    "    pred ,_,_= predict(inputs,\n",
    "                     position_ids=position_ids,\n",
    "                   attention_mask=attention_mask)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1bd6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_token_id = tokenizer.pad_token_id # 0\n",
    "sep_token_id = tokenizer.sep_token_id # 101\n",
    "cls_token_id = tokenizer.cls_token_id # 102\n",
    "ref_token_id,sep_token_id,cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d48d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意长度\n",
    "def truncate(ids,len_tru = 512):\n",
    "    return ids[:len_tru] if len(ids) > len_tru else ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8206c991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这是单个数据的处理方式，应该要想数据集应该怎么处理\n",
    "def construct_input_ref_pair(comment,AST_type,  ref_token_id, sep_token_id, cls_token_id):\n",
    "    comment = tokenizer.encode(comment, add_special_tokens=False,truncation=True,max_length=512)\n",
    "    AST_type = tokenizer.encode(AST_type, add_special_tokens=False,truncation=True,max_length=512)\n",
    "    # construct input token ids\n",
    "    input_ids = [cls_token_id] + comment + [sep_token_id] + AST_type + [sep_token_id]\n",
    "\n",
    "    # construct reference token ids \n",
    "    ref_input_ids = [cls_token_id] + [ref_token_id] * len(comment) + [sep_token_id] + \\\n",
    "        [ref_token_id] * len(AST_type) + [sep_token_id]\n",
    "    input_ids = truncate(input_ids)\n",
    "    ref_input_ids = truncate(ref_input_ids)\n",
    "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(comment)\n",
    "\n",
    "def construct_input_ref_token_type_pair(input_ids, sep_ind=0):\n",
    "    seq_len = input_ids.size(1)\n",
    "    token_type_ids = torch.tensor([[0 if i <= sep_ind else 1 for i in range(seq_len)]], device=device)\n",
    "    ref_token_type_ids = torch.zeros_like(token_type_ids, device=device)# * -1\n",
    "    return token_type_ids, ref_token_type_ids\n",
    "\n",
    "def construct_input_ref_pos_id_pair(input_ids):\n",
    "    seq_length = input_ids.size(1)\n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "    # we could potentially also use random permutation with `torch.randperm(seq_length, device=device)`\n",
    "    ref_position_ids = torch.zeros(seq_length, dtype=torch.long, device=device)\n",
    "\n",
    "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    return position_ids, ref_position_ids\n",
    "\n",
    "def construct_attention_mask(input_ids):\n",
    "    return torch.ones_like(input_ids)\n",
    "\n",
    "def construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                    token_type_ids=None, ref_token_type_ids=None, \\\n",
    "                                    position_ids=None, ref_position_ids=None):\n",
    "    input_embeddings = model.roberta.embeddings(input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "    ref_input_embeddings = model.roberta.embeddings(ref_input_ids, token_type_ids=ref_token_type_ids, position_ids=ref_position_ids)\n",
    "    \n",
    "    return input_embeddings, ref_input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0582b7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1094ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前k个贡献最高的word 和 token_type 和 position\n",
    "# return value为归因贡献值  indices为词对应的索引  top_tokens为 词或位置或token_type\n",
    "def get_topk_attributed_tokens(attrs,all_token_t, k=5):\n",
    "    values_max, indices_max = torch.topk(attrs, k)\n",
    "    top_tokens_max = [all_token_t[idx] for idx in indices_max]\n",
    "    values_min, indices_min = torch.topk(attrs, k, largest=False)\n",
    "    top_tokens_min = [all_token_t[idx] for idx in indices_min] \n",
    "    \n",
    "    return top_tokens_max, values_max,top_tokens_min,values_min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c77cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def split_punctuation(s):\n",
    "    # 使用正则表达式匹配连续的标点符号或者字母和标点符号之间的位置\n",
    "    splits = re.finditer(r'(?<=\\w)(?=[{}])|(?<=[{}])(?=\\w)'.format(string.punctuation, string.punctuation), s)\n",
    "    \n",
    "    # 获取所有分割位置\n",
    "    split_positions = [match.start() for match in splits]\n",
    "    \n",
    "    # 在分割位置插入空格\n",
    "    for pos in reversed(split_positions):\n",
    "        s = s[:pos] + ' ' + s[pos:]\n",
    "        \n",
    "    s = s.replace(\"< s >\", \"<s>\")\n",
    "    s = s.replace(\"</ s >\", \"</s>\")\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6453886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从 all_tokens 还原为 原单词 ，并且计算归因值\n",
    "def get_restore_words(code,comment,input_ids,all_tokens,attribution_num):\n",
    "    all_tokens_decode = tokenizer.decode(input_ids)\n",
    "    len_all_tokens_decode = len(all_tokens_decode) + 4\n",
    "    # 使用decode获得的序列，去掉分词之后的空格    例 ' a' -> 'a'\n",
    "    all_tokens_clean = []\n",
    "    for token in all_tokens:\n",
    "        s_without_leading_space = token.lstrip()\n",
    "        all_tokens_clean.append(s_without_leading_space)\n",
    "#     print('all_tokens_clean:\\n',all_tokens_clean)\n",
    "#     print('all_tokens_clean:\\n',len(all_tokens_clean))\n",
    "    \n",
    "\n",
    "    # 获得 code_comment_baseline\n",
    "    code = tokenizer.encode(code, add_special_tokens=False,truncation=True,max_length=512)\n",
    "    comment = tokenizer.encode(comment, add_special_tokens=False,truncation=True,max_length=512)\n",
    "    code_decode = tokenizer.decode(code)\n",
    "    comment_decode = tokenizer.decode(comment)\n",
    "    \n",
    "    code_comment_baseline = tokenizer.decode(tokenizer.cls_token_id) + ' '+ comment_decode \\\n",
    "                            + ' '+ tokenizer.decode(tokenizer.sep_token_id) + ' ' + code_decode \\\n",
    "                            + ' ' + tokenizer.decode(tokenizer.sep_token_id)\n",
    "    \n",
    "    code_comment_baseline = code_comment_baseline[:len_all_tokens_decode]\n",
    "    code_comment_baseline = split_punctuation(code_comment_baseline)\n",
    "    code_comment_baseline = code_comment_baseline.split()\n",
    "\n",
    "#     print('code_comment_baseline:\\n',code_comment_baseline)\n",
    "#     print('code_comment_baseline_len:\\n',len(code_comment_baseline))\n",
    "\n",
    "    \n",
    "    # 获得 相邻有几个token合并在一块的列表times  为了以后再计算attribute时求和\n",
    "    times = []\n",
    "    token_index = 0\n",
    "    for code_comment in code_comment_baseline:\n",
    "        temp = ''\n",
    "        time = 0\n",
    "        while temp != code_comment:\n",
    "            temp = temp + all_tokens_clean[token_index]\n",
    "            token_index = token_index + 1\n",
    "            time = time + 1\n",
    "        times.append(time)\n",
    "#     print('times:\\n',times)\n",
    "    \n",
    "    attribute_sum = []\n",
    "    start = 0\n",
    "    for time in times:\n",
    "        end = start + time\n",
    "        attribute = sum(attribution_num[start:end])\n",
    "        attribute_sum.append(attribute)\n",
    "        start = end\n",
    "#     print('attribute_sum:\\n',attribute_sum)\n",
    "    return code_comment_baseline ,attribute_sum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea788594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line_code_and_attribute(code,code_all_tokens,attributions_num):\n",
    "    # 特殊符号前  加空格\n",
    "    code = split_punctuation(code)\n",
    "    \n",
    "    # 把  行前空格去掉  便于单词与行之间匹配\n",
    "    code = [line.lstrip() for line in code.splitlines()]\n",
    "    code = '\\n'.join(code)  \n",
    "    code_lineList = code.split('\\n')\n",
    "    code_lineList = [' '.join(x.split()) for x in code_lineList]\n",
    "    # 有空行，把空行去掉\n",
    "    code_lineList = [item for item in code_lineList if item != '']\n",
    "    \n",
    "    attribute = []\n",
    "    i = 0  \n",
    "    for code_line in code_lineList:\n",
    "        if i < len(code_all_tokens):\n",
    "            temp = code_all_tokens[i]\n",
    "            attr = attributions_num[i]\n",
    "            i = i + 1\n",
    "        while((i < len(code_all_tokens)) and(temp != code_line)):\n",
    "            attr = attr + attributions_num[i]\n",
    "            temp = temp + ' ' + code_all_tokens[i]\n",
    "            i = i + 1\n",
    "\n",
    "        attribute.append(attr)\n",
    "        \n",
    "    for i in range(len(code_lineList)):\n",
    "        if len(code_lineList[i])== 1:\n",
    "            attribute[i] = torch.tensor(0, dtype=torch.float64)\n",
    "            \n",
    "    return code_lineList,attribute   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1432edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_before_and_including(lst, element):\n",
    "    if element in lst:\n",
    "        index = lst.index(element)\n",
    "        lst_c = lst[index+1:]\n",
    "        if element in lst_c:\n",
    "            lst_c.remove(element)\n",
    "        return lst_c\n",
    "    else:\n",
    "        return lst\n",
    "    \n",
    "def remove_after_including(lst, element):\n",
    "    if element in lst:\n",
    "        index = lst.index(element)\n",
    "        return lst[:index + 1]\n",
    "    else:\n",
    "        return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8635954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_all_attribute(code,all_tokens,attributions_num):\n",
    "    # 删除 all_token 列表中的<s>注释</s>  </s>\n",
    "    code_all_tokens = remove_before_and_including(all_tokens,'</s>')\n",
    "    comment_all_tokens = remove_after_including(all_tokens,'</s>')  \n",
    "    index = all_tokens.index('</s>')\n",
    "    attribute_comment = attributions_num[:index+1]\n",
    "#     print(attribute_comment)\n",
    "    \n",
    "    attribute_code = attributions_num[index+1:-1]\n",
    "#     print(attribute_code,len(attribute_code))\n",
    "    code_lineList_token,attribute_num_code = get_line_code_and_attribute(code,code_all_tokens,attribute_code)\n",
    "##     print(attribute_num_code)\n",
    "    attribute_num_code = torch.stack(attribute_num_code)\n",
    "    \n",
    "    new_all_tokens = comment_all_tokens + code_lineList_token\n",
    "    attributions_num_all = torch.cat((attribute_comment, attribute_num_code))\n",
    "\n",
    "#     attributions_num_all = attribute_comment + attribute_num_code\n",
    "#     print(len(attribute_comment),len(attribute_num_code))\n",
    "#     print(new_all_tokens)\n",
    "    return new_all_tokens,attributions_num_all,code_lineList_token,attribute_num_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450e5303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d196c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import LayerIntegratedGradients\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "lig = LayerIntegratedGradients(squad_pos_forward_func,input_embeddings)\n",
    "\n",
    "vis_data_records_ig = []\n",
    "\n",
    "def interpret_sentence(code,comment,old_code,input_ids,ref_input_ids, token_type_ids,\\\n",
    "                       position_ids, attention_mask, all_tokens, ground_lable,vis_data_records):\n",
    "    pre ,out,_ = predict(input_ids, position_ids=position_ids,attention_mask=attention_mask)\n",
    "    if out == 1:\n",
    "        sen_type = 'pos'\n",
    "    else:\n",
    "        sen_type = 'nag'\n",
    "    pre = pre.item()\n",
    "    pre = \"{:.3f}\".format(pre)\n",
    "    pre = float(pre) \n",
    "    pre ,sen_type\n",
    "    \n",
    "    attributions_ig, delta_ig = lig.attribute(input_ids, baselines=ref_input_ids,\\\n",
    "                           additional_forward_args=(position_ids,attention_mask,0),return_convergence_delta=True,internal_batch_size=8)\n",
    "    \n",
    "    attributions = attributions_ig.sum(dim=2).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "#     print(delta_ig)\n",
    "#     print(attributions)\n",
    "    all_tokens ,attributions = get_restore_words(code,comment,input_ids[0],all_tokens,attributions)     # 合并为一个单词\n",
    "    attributions = torch.tensor(attributions)\n",
    "    print(all_tokens)\n",
    "    # code_lineList_token 和 attribute_num_code 用于后续统计分析\n",
    "    new_all_tokens,attributions_num_all,code_lineList_token,attribute_num_code= final_all_attribute(code,all_tokens,attributions)\n",
    "    \n",
    "    add_attributions_to_visualizer(attributions_num_all, new_all_tokens, pre, ground_lable, sen_type, delta_ig, vis_data_records)\n",
    "\n",
    "    top_tokens_max, values_max,top_tokens_min,\\\n",
    "    values_min = get_topk_attributed_tokens(attributions_num_all,new_all_tokens)\n",
    "\n",
    "    return top_tokens_max, values_max,top_tokens_min,values_min,\\\n",
    "           code,old_code,code_lineList_token,attribute_num_code\n",
    "\n",
    "def add_attributions_to_visualizer(attributions, all_tokens, pre, ground_lable, sen_type, delta, vis_data_records):\n",
    "\n",
    "    # storing couple samples in an array for visualization purposes\n",
    "    vis_data_records.append(viz.VisualizationDataRecord(\n",
    "                            attributions,\n",
    "                            pre,\n",
    "                            pre,\n",
    "                            ground_lable,\n",
    "                            sen_type,\n",
    "                            attributions.sum(),\n",
    "                            all_tokens,\n",
    "                            delta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b270703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_sentence_2(code,comment,input_ids,ref_input_ids, token_type_ids,\\\n",
    "                         position_ids, attention_mask, all_tokens, ground_lable,vis_data_records):\n",
    "    pre ,out,_ = predict(input_ids,  position_ids=position_ids,attention_mask=attention_mask)\n",
    "    if out == 1:\n",
    "        sen_type = 'Inconsistency'\n",
    "    else:\n",
    "        sen_type = 'Consistency'\n",
    "    pre = pre.item()\n",
    "    pre = \"{:.3f}\".format(pre)\n",
    "    pre = float(pre) \n",
    "    pre ,sen_type\n",
    "    \n",
    "    attributions_ig, delta_ig = lig.attribute(input_ids, baselines=ref_input_ids,\\\n",
    "                           additional_forward_args=(position_ids,attention_mask,0),return_convergence_delta=True,internal_batch_size=8)\n",
    "    \n",
    "    attributions = attributions_ig.sum(dim=2).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "#     print(delta_ig)\n",
    "#     print(attributions)\n",
    "    try:\n",
    "        all_tokens ,attributions = get_restore_words(code,comment,input_ids[0],all_tokens,attributions)     # 合并为一个单词\n",
    "        attributions = torch.tensor(attributions)\n",
    "\n",
    "\n",
    "        # code_lineList_token 和 attribute_num_code 用于后续统计分析\n",
    "        new_all_tokens,attributions_num_all,code_lineList_token,attribute_num_code = final_all_attribute(code,all_tokens,attributions)        \n",
    "\n",
    "        add_attributions_to_visualizer(attributions_num_all, new_all_tokens, pre, ground_lable, sen_type, delta_ig, vis_data_records)\n",
    "\n",
    "        top_tokens_max, values_max, top_tokens_min,\\\n",
    "        values_min = get_topk_attributed_tokens(attributions_num_all,new_all_tokens)\n",
    "\n",
    "        return top_tokens_max, values_max, top_tokens_min,values_min,\\\n",
    "               code,code_lineList_token,attribute_num_code\n",
    "    except Exception as e:\n",
    "#         pass\n",
    "        print(\"解析错误\")\n",
    "        return _, _, _,_,_,_,_\n",
    "\n",
    "def add_attributions_to_visualizer(attributions, all_tokens, pre, ground_lable, sen_type, delta, vis_data_records):\n",
    "\n",
    "#     all_tokens = '\\n'.join(all_token for all_token in all_tokens)\n",
    "    \n",
    "#     all_tokens = (all_token+'\\n' for all_token in all_tokens)\n",
    "#     print(all_tokens)\n",
    "\n",
    "    # storing couple samples in an array for visualization purposes\n",
    "    vis_data_records.append(viz.VisualizationDataRecord(\n",
    "                            attributions,\n",
    "                            pre,\n",
    "                            sen_type,\n",
    "                            ground_lable,\n",
    "                            sen_type,\n",
    "                            attributions.sum(),\n",
    "                            all_tokens,\n",
    "                            delta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1bcfce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0e4a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_data_list(AST_list,comment_list):\n",
    "    input_ids_all = []\n",
    "    ref_input_ids_all = []\n",
    "    position_ids_all = []\n",
    "    attention_mask_all = []\n",
    "    token_type_ids_all = []\n",
    "    all_tokens_all = []\n",
    "    for i in range(len(AST_list)):\n",
    "        input_ids, ref_input_ids, comment_len = construct_input_ref_pair(comment_list[i],AST_list[i], ref_token_id,\\\n",
    "                                                                         sep_token_id, cls_token_id)\n",
    "        token_type_ids, ref_token_type_ids = construct_input_ref_token_type_pair(input_ids, comment_len)\n",
    "        position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n",
    "        attention_mask = construct_attention_mask(input_ids)\n",
    "        \n",
    "        indices = input_ids[0].detach().tolist()\n",
    "        \n",
    "        all_tokens = []                                       ###\n",
    "        for _, token in enumerate(indices):\n",
    "            all_tokens.append(tokenizer.decode([token]))\n",
    "        \n",
    "        input_ids_all.append(input_ids)\n",
    "        ref_input_ids_all.append(ref_input_ids)\n",
    "        position_ids_all.append(position_ids)\n",
    "        attention_mask_all.append(attention_mask)\n",
    "        token_type_ids_all.append(token_type_ids)\n",
    "        all_tokens_all.append(all_tokens)\n",
    "\n",
    "    return input_ids_all,ref_input_ids_all,position_ids_all,attention_mask_all,token_type_ids_all,all_tokens_all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f82b9b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19bc1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_other_index(df,index):\n",
    "    filtered_df = df.loc[index]\n",
    "    filtered_df = filtered_df.reset_index(drop=True)\n",
    "    filtered_df[\"raw_data_index_column\"] = index\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13cd3de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3235d372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_train_data():\n",
    "    test_param = pd.read_json(\"D:/BERT_learing/code_comment_inconsistency_detection/data/param/train.json\")\n",
    "    test_return = pd.read_json(\"D:/BERT_learing/code_comment_inconsistency_detection/data/return/train.json\")\n",
    "    test_summary = pd.read_json(\"D:/BERT_learing/code_comment_inconsistency_detection/data/summary/train.json\")\n",
    "    test_df = pd.concat([test_summary,test_param, test_return], axis=0)\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "    return test_df\n",
    "train_df = retrieve_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3270e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_last(df):\n",
    "    for i in range(len(df)):\n",
    "        string = df.loc[i]['old_comment_raw']\n",
    "        df['old_comment_raw'][i] = string.rstrip('.')\n",
    "    return df\n",
    "train_df = remove_last(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05be6f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6204d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_false_index_ast = [610,716, 363]\n",
    "test_df_true_set = remove_other_index(train_df,list_false_index_ast)\n",
    "code_list = list(test_df_true_set['new_code_raw'])\n",
    "comment_list = list(test_df_true_set['old_comment_raw'])\n",
    "ground_lable = list(test_df_true_set['label'])\n",
    "\n",
    "input_ids_all,ref_input_ids_all,position_ids_all,\\\n",
    "attention_mask_all,token_type_ids_all,all_tokens_all= input_data_list(code_list,comment_list)\n",
    "\n",
    "vis_data_records_ig = []\n",
    "for i in range(len(code_list)):  \n",
    "    top_tokens_max, values_max, top_tokens_min,values_min,code,_,_ \\\n",
    "    = interpret_sentence_2(code_list[i],comment_list[i],input_ids_all[i],ref_input_ids_all[i],\\\n",
    "                           token_type_ids_all[i], position_ids_all[i], attention_mask_all[i], all_tokens_all[i],\\\n",
    "                           ground_lable[i],vis_data_records_ig)\n",
    "    \n",
    "    \n",
    "print('Visualize attributions based on Integrated Gradients')\n",
    "_ = viz.visualize_text(vis_data_records_ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6114af",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_false_index_ast = [65]\n",
    "test_df_true_set = remove_other_index(test_df,list_false_index_ast)\n",
    "code_list = list(test_df_true_set['new_code_raw'])\n",
    "comment_list = list(test_df_true_set['old_comment_raw'])\n",
    "ground_lable = list(test_df_true_set['label'])\n",
    "\n",
    "input_ids_all,ref_input_ids_all,position_ids_all,\\\n",
    "attention_mask_all,token_type_ids_all,all_tokens_all= input_data_list(code_list,comment_list)\n",
    "\n",
    "vis_data_records_ig = []\n",
    "for i in range(len(code_list)):  \n",
    "    top_tokens_max, values_max, top_tokens_min,values_min,code,_,_ \\\n",
    "    = interpret_sentence_2(code_list[i],comment_list[i],input_ids_all[i],ref_input_ids_all[i],\\\n",
    "                           token_type_ids_all[i], position_ids_all[i], attention_mask_all[i], all_tokens_all[i],\\\n",
    "                           ground_lable[i],vis_data_records_ig)\n",
    "    \n",
    "    \n",
    "print('Visualize attributions based on Integrated Gradients')\n",
    "_ = viz.visualize_text(vis_data_records_ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35373fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leitx",
   "language": "python",
   "name": "leitx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
